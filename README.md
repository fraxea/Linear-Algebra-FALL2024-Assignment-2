# Applications of Linear Algebra

In this repository, we design a series of notebooks to explore some applications of *Linear Algebra* in Machine Learning.

![DALL-E Generated Image](DALLÂ·E%202024-12-07%2020.17.06%20-%20A%20visually%20engaging%20illustration%20representing%20the%20application%20of%20linear%20algebra%20in%20machine%20learning,%20highlighting%20specific%20techniques%20like%20matrix%20oper.webp)

## Neural Network Intro

Some techniques like matrix operations and derivatives are the core in neural network architectures.

- Apply Linear Regression to predict the waiting time given the number of cars ahead of you in gas station.
- Get familiar with gradient descent, backpropagation, and other prerequisties of Neural Network.
- Implement RNN from scratch for Language Modeling on Harry Potter books.

![RNN architecture](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc)

## Word Embeddings

Representing words as vectors to maintain their meaning is a big idea. Some linear algebra based techniques were used to learn these word vectors (embeddings).

- Get familiar with GloVe. Evaluate word embeddings.
- Learn embeddings with SVD.
- Use these embeddings for sentiment analysis.

![comparative - superlative](https://nlp.stanford.edu/projects/glove/images/comparative_superlative.jpg)

## Recommendation Systems

Matrix factorization is the key in recommenders based on SVD.

- Use SVD to apply collaborative filtering on movielens dataset.

![Collaborative Filtering](https://media.licdn.com/dms/image/v2/C4E12AQGnmr-VQ1zM7g/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1625459218650?e=1737590400&v=beta&t=8tnllRgtpZXZJ3foOLa_nmUWIp5jFUBgFQTBs_I4ULM)

## Refrences

1. [Recurrent Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
2. [GloVe: Global Vectors for Word Representation](https://github.com/stanfordnlp/GloVe)
3. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)
4. [Evaluating word embedding models: methods and experimental results](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/EDF43F837150B94E71DBB36B28B85E79/S204877031900012Xa.pdf/div-class-title-evaluating-word-embedding-models-methods-and-experimental-results-div.pdf)
5. [Neural Collaborative Filtering](https://www.nvidia.com/en-us/glossary/recommendation-system)
6. [Surprise Singular Value Decomposition](https://github.com/recommenders-team/recommenders/blob/main/examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb)
